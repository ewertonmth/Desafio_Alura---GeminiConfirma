{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPp2AQO9Zdv0LV4VGisn5dW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ewertonmth/Desafio_Alura---GeminiConfirma/blob/main/GeminiConfirma2_0_public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalação e configuração do Gemini 2.0 Flash no Google Colab\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configuração com a sua chave da API\n",
        "genai.configure(api_key=\"SUA_API_KEY\")  # ← Substitua por sua chave!\n",
        "\n",
        "# Inicializa o modelo Gemini 2.0 Flash\n",
        "modelo = genai.GenerativeModel(model_name=\"gemini-2.0-flash\")\n",
        "\n",
        "# Teste de conexão com o modelo\n",
        "try:\n",
        "    resposta = modelo.generate_content(\"Diga 'ok' se você está funcionando corretamente.\")\n",
        "    print(\"✅ Conectado com sucesso ao Gemini 2.0 Flash!\")\n",
        "    print(\"Resposta de teste:\", resposta.text)\n",
        "except Exception as e:\n",
        "    print(\"❌ Erro ao conectar à API do Gemini:\", e)"
      ],
      "metadata": {
        "id": "faRSL1e2ynM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joAe54CxxYOp"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Função para buscar no Google Custom Search (adicione suas credenciais)\n",
        "GOOGLE_CSE_ID = \"SUA_CSE_ID\" # ← Substitua por sua chave!\n",
        "\n",
        "def google_search(query, api_key=GOOGLE_API_KEY, cse_id=GOOGLE_CSE_ID, num_results=3):\n",
        "    \"\"\"\n",
        "    Realiza uma busca no Google Custom Search e retorna os títulos e links dos principais resultados.\n",
        "    \"\"\"\n",
        "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "    params = {\n",
        "        \"key\": api_key,\n",
        "        \"cx\": cse_id,\n",
        "        \"q\": query,\n",
        "        \"num\": num_results\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        items = response.json().get(\"items\", [])\n",
        "        resultados = []\n",
        "        for item in items:\n",
        "            titulo = item.get(\"title\")\n",
        "            link = item.get(\"link\")\n",
        "            resultados.append(f\"{titulo}: {link}\")\n",
        "        return \"\\n\".join(resultados) if resultados else \"Nenhum resultado encontrado.\"\n",
        "    except Exception as e:\n",
        "        return f\"Erro na busca Google: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de requisição ao Gemini com temperatura\n",
        "def gemini_request(prompt, model=modelo, temperature=0.7):\n",
        "        \"\"\"\n",
        "        Envia um prompt para o modelo Gemini e retorna a resposta.\n",
        "        Permite configurar a temperatura para ajustar o nível de criatividade.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = model.generate_content(\n",
        "                prompt,\n",
        "                generation_config={\"temperature\": temperature}\n",
        "            )\n",
        "            return response.text.strip()\n",
        "        except Exception as e:\n",
        "            return f\"[Erro na requisição Gemini: {e}]\""
      ],
      "metadata": {
        "id": "RnRC8uLoxjxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agente 1: Geração do resumo (temperatura 0.3)\n",
        "def gerar_resumo(texto, model=modelo):\n",
        "    prompt = f\"\"\"\n",
        "    Sua tarefa é resumir o seguinte conteúdo de forma objetiva e clara,\n",
        "    preservando os fatos principais e removendo opiniões ou exageros.\n",
        "\n",
        "    Texto original:\n",
        "    \\\"\\\"\\\"{texto}\\\"\\\"\\\"\n",
        "\n",
        "    Resuma o conteúdo acima em no máximo 5 linhas:\n",
        "    \"\"\"\n",
        "    return gemini_request(prompt, model=model, temperature=0.3)"
      ],
      "metadata": {
        "id": "_IBSmPVBxoTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verificar_fatos(resumo, model=modelo):\n",
        "        # 1. Realiza a busca no Google usando a sua função externa\n",
        "        resultados_busca = google_search(resumo) # Usa sua função google_search e GOOGLE_CSE_ID\n",
        "\n",
        "        # 2. Monta o prompt INCLUINDO os resultados da busca\n",
        "        prompt = f\"\"\"\n",
        "        Você é um verificador de fatos. Analise o seguinte resumo de uma notícia e diga se ele apresenta informações\n",
        "        verdadeiras, falsas ou duvidosas com base no conhecimento geral, nas fontes confiáveis listadas abaixo e em informações públicas.\n",
        "\n",
        "        Resumo da notícia:\n",
        "        \\\"\\\"\\\"{resumo}\\\"\\\"\\\"\n",
        "\n",
        "        Resultados da busca no Google:\n",
        "        {resultados_busca} # Os resultados da busca são adicionados aqui\n",
        "\n",
        "        Diga se o conteúdo parece confiável, falso ou impreciso. Justifique sua análise de forma objetiva.\n",
        "        \"\"\"\n",
        "        # 3. Envia o prompt COMPLETO (com os resultados da busca) para o Gemini\n",
        "        return gemini_request(prompt, model=model, temperature=0.7)"
      ],
      "metadata": {
        "id": "GtrwQZRrxqgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agente 3: Análise de linguagem manipulativa (temperatura 0.7)\n",
        "def analisar_linguagem(texto, model=modelo):\n",
        "    prompt = f\"\"\"\n",
        "    Analise o seguinte texto e identifique se ele contém linguagem manipulativa, sensacionalista ou tendenciosa,\n",
        "    como apelos emocionais, exageros ou tentativa de influenciar o leitor.\n",
        "\n",
        "    Texto:\n",
        "    \\\"\\\"\\\"{texto}\\\"\\\"\\\"\n",
        "\n",
        "    Indique os principais pontos de manipulação encontrados e explique brevemente.\n",
        "    \"\"\"\n",
        "    return gemini_request(prompt, model=model, temperature=0.7)"
      ],
      "metadata": {
        "id": "2jRzhS4Zxsex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agente 4: Geração do parecer final (temperatura 0.8)\n",
        "def gerar_parecer_final(resumo, verificacao, linguagem, model=modelo):\n",
        "    prompt = f\"\"\"\n",
        "    Com base nas informações a seguir, faça um parecer final sobre a confiabilidade da notícia,\n",
        "    usando linguagem clara e objetiva.\n",
        "\n",
        "    Resumo da notícia:\n",
        "    \\\"\\\"\\\"{resumo}\\\"\\\"\\\"\n",
        "\n",
        "    Verificação factual:\n",
        "    \\\"\\\"\\\"{verificacao}\\\"\\\"\\\"\n",
        "\n",
        "    Análise de linguagem:\n",
        "    \\\"\\\"\\\"{linguagem}\\\"\\\"\\\"\n",
        "\n",
        "    Parecer final:\n",
        "    \"\"\"\n",
        "    return gemini_request(prompt, model=model, temperature=0.8)"
      ],
      "metadata": {
        "id": "65IgOxO7xuOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função que integra o fluxo completo\n",
        "def analisar_texto_completo(texto):\n",
        "    resumo = gerar_resumo(texto)\n",
        "    verificacao = verificar_fatos(resumo)\n",
        "    linguagem = analisar_linguagem(texto)\n",
        "    parecer = gerar_parecer_final(resumo, verificacao, linguagem)\n",
        "\n",
        "    return {\n",
        "        \"Resumo\": resumo,\n",
        "        \"Verificação Factual\": verificacao,\n",
        "        \"Análise de Linguagem\": linguagem,\n",
        "        \"Parecer Final\": parecer\n",
        "    }"
      ],
      "metadata": {
        "id": "Nvi8WU5oxwEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrada do usuário\n",
        "texto_teste = input(\"Digite o texto(ou link) da notícia ou conteúdo para análise:\\n\")\n",
        "\n",
        "# Executa a análise\n",
        "resultado = analisar_texto_completo(texto_teste)\n",
        "\n",
        "# Exibe o resultado formatado\n",
        "for etapa, texto in resultado.items():\n",
        "    print(f\"\\n--- {etapa} ---\\n{texto}\")"
      ],
      "metadata": {
        "id": "CWM2FSH6xxta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}